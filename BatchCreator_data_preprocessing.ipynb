{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D,Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "from IPython.display import display \n",
    "#from PIL import Image\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from PIL import Image, ImageFilter\n",
    "from keras import backend as k\n",
    "config = tf.ConfigProto()\n",
    "    \n",
    "#BASE_DIR='/tmp'\n",
    "S3_DATA_BUCKET_NAME='cop-group10'\n",
    "DATASET_NAME='training-data'\n",
    "train_data_dir = DATASET_NAME\n",
    "#train_data_dir = BASE_DIR+'/'+DATASET_NAME\n",
    "validation_data_dir = DATASET_NAME\n",
    "#validation_data_dir = BASE_DIR+'/'+DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "An error occurred (AccessDenied) when calling the GetUser operation: User: arn:aws:sts::716282482850:assumed-role/test-stack-codebuild-role/SageMaker is not authorized to perform: iam:GetUser on resource: user SageMaker\r\n"
     ]
    }
   ],
   "source": [
    "!aws iam get-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed once to sync data. Otherwise it fails to access the S3 storage.\n",
    "!aws s3 sync s3://$S3_DATA_BUCKET_NAME/$DATASET_NAME $DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images found in this directory 300\n",
      "pikle....\n",
      "pikle....\n",
      "pikle....\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "starttime = time.time()\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 180,320#720, 1280#180, 320\n",
    "\n",
    "\n",
    "nb_train_samples = 10\n",
    "nb_validation_samples = 100\n",
    "nb_epoch = 1\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import skimage \n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage.color import rgb2gray\n",
    "flag = 1\n",
    "\n",
    "imagesName = os.listdir(train_data_dir)\n",
    "\n",
    "print('No of images found in this directory', len(imagesName))\n",
    "count = 0\n",
    "filecount = 0\n",
    "tempImages = []\n",
    "for image in imagesName:\n",
    "    cap = io.imread(train_data_dir + \"/\" +image)\n",
    "    cap = rescale(cap, .25, anti_aliasing=False)\n",
    "    tempImages.append(cap)\n",
    "    count = count + 1\n",
    "    if(count == 100):\n",
    "       count = 0\n",
    "       tempImages = np.array(tempImages)\n",
    "       tempImages = tempImages.reshape(tempImages.shape[0],-1)\n",
    "       filename = \"pickle\"+str(filecount)+\".pk\"\n",
    "       file = open(\"pickle/\"+filename,\"wb\")\n",
    "       pk.dump(tempImages,file)\n",
    "       filecount = filecount + 1\n",
    "       tempImages = []\n",
    "       print(\"pikle....\")\n",
    "       file.close()\n",
    "        \n",
    "print('Finished!')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
